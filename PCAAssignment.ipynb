{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tExplain the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience\n",
    "\n",
    "The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is a dimensionality reduction technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Dimensionality reduction is a feature selection technique using which we reduce the number of features to be used for making a model without losing a significant amount of information compared to the original dataset. In other words, a dimensionality reduction technique projects a data of higher dimension to a lower-dimensional subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tExplain PCA. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => The principal component analysis is an unsupervised machine learning algorithm used for feature selection using dimensionality reduction techniques. As the name suggests, it finds out the principal components from the data. PCA transforms and fits the data from a higher-dimensional space to a new, lower-dimensional subspace\n",
    "\n",
    "What are the principal components? Principal components are the derived features which explain the maximum variance in the data. The first principal component explains the most variance, the 2nd a bit less and so on. Each of the new dimensions found using PCA is a linear combination of the old features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat is explained variance ratio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => It represents the amount of variance each principal component is able to explain. For example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 it’s 5.\n",
    "\n",
    "EVR of PC1=$\\frac{Distance of PC1 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{50}{55}=0.91 $\n",
    "\n",
    "EVR of PC2=$\\frac{Distance of PC2 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{5}{55}=0.09 $\n",
    "\n",
    "\n",
    "Thus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n",
    "\n",
    "In a real-life scenario, this problem is solved using the **Scree Plots**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhat is a scree plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Scree plots are the graphs that convey how much variance is explained by corresponding Principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tHow is the optimum number of principal components obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => \n",
    "Let’s plot this on the XY plane and calculate the average of the magnitude of all the points. Blue ones are the actual points and the yellow one is the average point.\n",
    "\n",
    "Move the points so that the average point is on the origin. This is called a parallel translation. Although the coordinates of the points have changed, the corresponding distances among them remain the same.\n",
    "\n",
    "Create the best fit line for the new data points. We first start with a random line(blue one), and then try to find the best fit line(the green one) so that the distance from individual data points is minimum and consequently the distance from origin is maximum. This best fit line is called Principal component1 or PC1.\n",
    "\n",
    "PC2 is a line perpendicular to the PC1.\n",
    "\n",
    "Then the axes PC1 and PC2 are rotated in a way that PC1 becomes the horizontal axis.\n",
    "\n",
    "Then based on the sample points the new points are projected using PC1 and PC2. Thus we get the derived features.\n",
    "\n",
    "if we talk about n dimensions, there are n-1 perpendicular lines possible on PC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat is covariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Variance is the measure of how a variable changes or varies and co means together. Hence, covariance is the measure of how two variables change together.\n",
    "If the covariance is high, it means that the variables are highly correlated and change in one results in a change in the other one too. Generally, we avoid using highly correlated variables in building a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is the transpose of a matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => The transpose of a matrix is simply a flipped version of the original matrix. We can transpose a matrix by switching its rows with its columns. We denote the transpose of matrix A by AT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is an Eigen value and Eigen Vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => An **eigenvector v** of a linear transformation **T** is a nonzero vector that, when **T** is applied to it, does not change direction. Applying __T__ to the eigenvector only scales the eigenvector by the scalar value λ, called an **eigenvalue**. This condition can be written as the equation\n",
    "\n",
    "$$\n",
    "{\\displaystyle T(\\mathbf {v} )=\\lambda \\mathbf {v} ,} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhy the Principal Components are orthogonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => If the dot product of two matrices is zero, then they are considered to be orthogonal\n",
    "\n",
    "Because the second Principal Component should capture the highest variance from what is left after the first Principal Component explains the data as much as it can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tExplain the Eigen Decomposition approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => \n",
    "### Summary of Eigen-decomposition Approach\n",
    "1. Normalize columns of $A$ so that each feature has a mean of zero\n",
    "1. Compute sample covariance matrix $\\Sigma = {A^TA}/{(m-1)}$\n",
    "1. Perform eigen-decomposition of $\\Sigma$ using `np.linalg.eig(Sigma)`\n",
    "1. Compress by ordering $k$ evectors according to largest e-values and compute $AX_k$\n",
    "1. Reconstruct from the compressed version by computing $A X_k X_k^T$\n",
    "\n",
    "Magically, eigen-decomposition (or PCA) finds the line where\n",
    "1. the spread of values along the black line is **maximal**\n",
    "2. the projection error (sum of red lines) is **minimal**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tHow can PCA be used for data compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => PCA can be used for a sort of data compression as well. Using a smaller value of n_components allows you to represent a higher dimensional point as a sum of just a few principal component vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the pros and cons of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans =>\n",
    "**Pros of PCA:**\n",
    "\n",
    "- Correlated features are removed.\n",
    "- Model training time is reduced.\n",
    "- Overfitting is reduced.\n",
    "- Helps in better visualizations\n",
    "- Ability to handle noise\n",
    "\n",
    "**Cons of PCA**\n",
    "- The resultant principal components are less interpretable than the original data\n",
    "- Can lead to information loss if the explained variance threshold is not considered appropriately.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
