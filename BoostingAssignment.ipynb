{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Boosting is an ensemble approach(meaning it involves several trees) that starts from a weaker decision and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Bagging helps to decrease the model’s variance.While the training stage is parallel for Bagging (i.e., each model is built independently), Boosting builds the new learner in a sequential way as follows:\n",
    "Boosting helps to decrease the model’s bias. In Boosting algorithms each classifier is trained on data, taking into account the previous classifiers’ success.\n",
    "\n",
    "After each training step, the weights are redistributed. Misclassified data increases its weights to emphasise the most difficult cases.\n",
    "\n",
    "In this way, subsequent learners will focus on them during their training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Weak learner(classifer, predictor, etc) is just one which performs relatively poorly for prediction.\n",
    "Weak learner also suggests that many instances of the algorithm are being pooled (via boosting, bagging, etc) together into to create a \"strong\" ensemble classifier.\n",
    "\n",
    "The classification will be much more proficient if multiple classifiers are combined and incorporated. The AdaBoost classifier is one such classifier, in which multiple weak classifiers are combined to form a strong classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Each tree attempts to minimize the errors of previous tree. Trees in boosting are weak learners but adding many trees in series and each focusing on the errors from previous one make boosting a highly efficient and accurate model. Unlike bagging, boosting does not involve bootstrap sampling. Everytime a new tree is added, it fits on a modified version of initial dataset.\n",
    "\n",
    "Since trees are added sequentially, boosting algorithms learn slowly. In statistical learning, models that learn slowly perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => 1) suppose There are a total of 8 rows in our dataset. Hence, we’ll initialize the sample weights( 𝑤=1𝑁 ) as 1/8 in the beginning. And, at the beginning, all the samples are equally important.\n",
    "\n",
    "2) We’ll consider the individual columns to create weak decision-makers as shown below and then try to figure out what are the correct and incorrect predictions based on that column\n",
    "\n",
    "3) We’ll now calculate the Gini index of the individual stumps using the formula\n",
    "\n",
    "G.I=  ∑(𝑤𝑒𝑖𝑔ℎ𝑡𝑜𝑓𝑡ℎ𝑒𝑑𝑒𝑐𝑖𝑠𝑖𝑜𝑛)∗(1−(𝑝2+(1−𝑝)2))\n",
    "\n",
    "And, we select the tree with the lowest Gini Index. This will be the first decision-maker for our model.\n",
    "\n",
    "4)Now, we’ll calculate the contribution of this tree(stump) to our final decision using the formula:\n",
    "\n",
    "Contribution=  ½(𝑙𝑜𝑔(1−𝑡𝑜𝑡𝑎𝑙𝑒𝑟𝑟𝑜𝑟)/𝑡𝑜𝑡𝑎𝑙𝑒𝑟𝑟𝑜𝑟)\n",
    "\n",
    "As this stump classified only one data incorrectly out of the 8, hence the total error is 1/8.\n",
    "\n",
    "\n",
    "5) We’ll now calculate the new weights using the formula:\n",
    "\n",
    "Increase the sample weight for incorrectly classified datapoints New weight= old weighte^ contribution= 1/8 e^0.97=0.33\n",
    "\n",
    "Decrease the sample weight for incorrectly classified datapoints New weight= old weighte^- contribution= 1/8 e^-0.97=0.05\n",
    "\n",
    "Populate the new weights and Normalize the sample weights\n",
    "\n",
    "Then we create new trees which consider the dataset which was prepared using the new sample weights.\n",
    "\n",
    "Suppose, m trees(stumps) are classifying a person as a heart patient and n trees(stumps) are classifying a person as a healthy one, then the contribution of m and n trees are added separately and whichever has the higher value, the person is classified as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => The initial guess of the Gradient Boosting algorithm is to predict the average value of the target y . ... For the variable x1 , we compute the difference between the observations and the prediction we made. This is called the pseudo-residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans=>\n",
    "- Calculate the average of the label column as initially this average shall minimise the total error.\n",
    "- Calculate the pseudo residuals.\n",
    "       Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    "  Mathematically,\n",
    "  \n",
    "     derivative of the pseudo residual=$(\\frac {\\delta L(y_i,f(x_i))}{\\delta (f(x_i))})$\n",
    "     \n",
    "     where, L is the loss function.\n",
    "                          \n",
    "               \n",
    "     Here, the gradient of the error term is getting calculated as the goal is to minimize the error. Hence the name gradient boosted trees\n",
    "- create a tree to predict the pseudo residuals instead  of a tree to predict for the actual column values.\n",
    "- new result= previous result+learning rate* residual \n",
    "   \n",
    "   Mathematically, \n",
    "     $ F_1(x)= F_0(x)+ \\nu \\sum \\gamma $\n",
    "     \n",
    " where  $ \\nu $ is the learning rate and $ \\gamma $ is the residual\n",
    "\n",
    "Repeat these steps until the residual stops decreasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :- \n",
    "XGBoost's objective function is the sum of loss function evaluated over all the predictions and a regularisation function for all predictors ( 𝑗  trees). In the formula  𝑓𝑗  means a prediction coming from the  𝑗𝑡ℎ  tree.\n",
    "\n",
    "𝑜𝑏𝑗(𝜃)=∑𝑖𝑛𝑙(𝑦𝑖−𝑦𝑖^)+∑𝑗=1𝑗Ω(𝑓𝑗)\n",
    " \n",
    "Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n",
    "\n",
    "Ω(𝑓)=𝛾𝑇+12𝜆∑𝑗=1𝑇𝑤2𝑗\n",
    " \n",
    "First part ( 𝛾𝑇 ) is responsible for controlling the overall number of created leaves, and the second term ( 12𝜆∑𝑇𝑗=1𝑤2𝑗 ) watches over the scores.\n",
    "Mathematics Involved Unlike the other tree-building algorithms, XGBoost doesn’t use entropy or Gini indices. Instead, it utilises gradient (the error term) and hessian for creating the trees\n",
    "\n",
    "where L is the loss function.\n",
    "\n",
    "Initialise the tree with only one leaf.\n",
    "compute the similarity using the formula\n",
    "𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦=𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡2ℎ𝑒𝑠𝑠𝑖𝑎𝑛+𝜆\n",
    " \n",
    "Where  𝜆  is the regularisation term.\n",
    "Now for splitting data into a tree form, calculate\n",
    "𝐺𝑎𝑖𝑛=𝑙𝑒𝑓𝑡𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦+𝑟𝑖𝑔ℎ𝑡𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦−𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦𝑓𝑜𝑟𝑟𝑜𝑜𝑡\n",
    " \n",
    "For tree pruning, the parameter  𝛾  is used. The algorithm starts from the lowest level of the tree and then starts pruning based on the value of  𝛾 .\n",
    "If  𝐺𝑎𝑖𝑛−𝛾<0 , remove that branch. Else, keep the branch\n",
    "\n",
    "Learning is done using the equation\n",
    "𝑁𝑒𝑤𝑉𝑎𝑙𝑢𝑒=𝑜𝑙𝑑𝑉𝑎𝑙𝑢𝑒+𝜂∗𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans -:\n",
    "**The main advantages:**\n",
    "- out of the box feature of appropriate bias-variance trade-off,\n",
    "- great computation speed as it utilises parallel computing and cache optimization,\n",
    "- uses hardware optimization,\n",
    "- works well even if the features are correlated\n",
    "- robust even if there is noise for classification problem\n",
    "- the facility of early stopping\n",
    "- the package is evolving, i.e., new features are being added."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
