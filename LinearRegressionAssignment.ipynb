{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans=> The regression might be used to identify the strength of the effect that the independent variable(s) have on a dependent variable. Typical questions are what is the strength of relationship between dose and effect, for eg sales and marketing spending, or age and income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Linear Regression is a linear approch to modeling the relationship between a dependent varible and one or more independent varible, commonly used type of predictive analysis. \n",
    "The overall idea of regression is to examine two things: \n",
    "(1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable?  \n",
    "(2) Which variables in particular are significant predictors of the outcome variable, and in what way do they–indicated by the magnitude and sign of the beta estimates–impact the outcome variable?  \n",
    "The simplest form of the regression equation with one dependent and one independent variable is defined by the formula y = c + b*x, where y = estimated dependent variable score, c = constant, b = regression coefficient, and x = score on the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhen to use Linear Regression? Explain the equation of a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Use regression analysis to describe the relationships between a set of independent variables and the dependent variable \n",
    "1) Discreet/continuous independent variables\n",
    "2) A best-fit regression line\n",
    "3) Continuous dependent variable. i.e. A Linear Regression model predicts the dependent variable using a regression line based on the independent variables. The equation of the Linear Regression is: Y=a+b*X + e\n",
    "Where, a is the intercept, b is the slope of the line, and e is the error term. The equation above is used to predict the value of the target variable based on the given predictor variable(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat kind of plots will you use to showcase the relationship amongst the columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => \n",
    "1) Scatter plots : are used to determine the relationship between two variables\n",
    "2) Jointplot : is seaborn library specific and can be used to quickly visualize and analyze the relationship between two variables\n",
    "3) PairPlot : if the dataset has more than two variables (which is quite often the case), it can be a tedious task to visualize the relationship between each variable with the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow is the best fit line chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => using the least-squares criterion, i.e., the best fit line has to be calculated that minimizes the sum of squared residuals, Residual is the distance between the actual Y and the predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat is gradient descent, and why is it used? Explain the maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => A gradient measures how much the output of a function changes if you change the inputs a little bit, Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent\n",
    "\n",
    "The equation b = a - gamma * Δf(a)  describes what gradient descent does: b is the next position of our climber, while a represents his current position. The minus sign refers to the minimization part of gradient descent. The gamma in the middle is a waiting factor and the gradient term (Δf(a)) is simply the direction of the steepest descent.\n",
    "So this formula basically tells us the next position we need to go, which is the direction of the steepest descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat are residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => As residuals are the differences between observed and predicted values of data, they are sometimes called “errors.” Error in this context doesn’t mean that there’s something wrong with the analysis; it just means that there is some unexplained difference. In other words, the residual is the error that isn’t explained by the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables (e.g., height, weight). This will define positive and negative correlations between these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Multi-collinearity is the statistical term to represent type of a relation amongst the independent variable- when the independent variables are not so independent\n",
    "We can define multi-collinearity as the situation where the independent variables (or the predictors) have strong correlation amongst themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tHow to detect multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Multicollinearity can also be detected with the help of tolerance and its reciprocal, called variance inflation factor (VIF). If the value of tolerance is less than 0.2 or 0.1 and, simultaneously, the value of VIF 10 and above, then the multicollinearity is problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tWhat are the remedies for multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Do Nothing: If the Correlation is not that extreme, we can ignore it. If the correlated variables are not used in solving our business question, they can be ignored.\n",
    "Remove One Variable: Like in dummy variable trap\n",
    "Combine the correlated variables: Like creating a seniority score based on Age and Years of experience\n",
    "Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat is the R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared statistic provides a measure of fit. It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1. In simple words, it represents how much of our data is being explained by our model. For example,  𝑅2  statistic = 0.75, it says that our model fits 75 % of the total data set. Similarly, if it is 0, it means none of the data points is being explained and a value of 1 represents 100% data explanation. Mathematically  𝑅2  statistic is calculated as : R-sqaured = 1 - RSS / TSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tWhat is an adjusted R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => As we increase the number of independent variables in our equation, the R2 increases as well. But that doesn’t mean that the new independent variables have any correlation with the output variable. In other words, even with the addition of new features in our model, it is not necessary that our model will yield better results but R2 value will increase. To rectify this problem, we use Adjusted R2 value which penalises excessive use of such features which do not correlate with the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.\tWhy do we use adj R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Adjusted R-squared is used to determine how reliable the correlation is and how much is determined by the addition of independent variables, we use Adjusted R2 value which penalises excessive use of such features which do not correlate with the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhy adj R-squared decreases when we use incompetent variables?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Adjusted R-squared is used to determine how reliable the correlation is and how much is determined by the addition of independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tHow to interpret a Linear Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable the dependent variable. A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tWhat is the difference between fit, fit_transform and predict methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit() : method takes the training data as arguments to fit the model, computes the mean and std to be used for later scaling.\n",
    "fit_transform() : joins these two steps and is used for the initial fitting of parameters on the training set x, while also returning the transformed x′. Internally, the transformer object just calls first fit() and then transform() on the same data.\n",
    "predict() :  method that, given unlabeled observations X, returns the predicted labels y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tHow do you plot the least squared line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => First we do scatter plot between x and y : data.plot(kind='scatter', x='X', y='Y')\n",
    "then, plot the least squares line between x and prediction(Y) : plt.plot(X, Y_pred, c='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.\tWhat are Bias and Variance? What is Bias Variance Trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Variance is a measure of the degree of the spread of the predicted results. Bias, on the other hand, is a measure of the polarity of the results i.e., the results are favored towards a specific outcome\n",
    "It can be seen that as the bias decreases, the variance increases. So, we need to find a balance so that we get a model which has low variance as well as low bias. This is called bias-variance tradeoff.\n",
    "\n",
    "Bias can be minimized by training with more data and variance can be reduced by using regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tWhat is the null and alternate hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Null hypothesis: No relationship exists between X and Y (and hence  𝛽1  equals zero).\n",
    "Alternative hypothesis: There exists a relationship between X and Y (and hence,  𝛽1  is not equal to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.\tWhat is multiple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => we’ll include multiple features and create a model to see the relationship between those features and the label column. This is called Multiple Linear Regression.\n",
    "\n",
    "𝑦=𝛽0+𝛽1𝑥1+...+𝛽𝑛𝑥𝑛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is the OLS method? Derive the formulae used in the OLS method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans=> ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function\n",
    "\n",
    "Suppose you are given a set of data points {(xi,yi)}. The goal of linear regression is to find a line that minimizes the sum of square of errors at each xi. Let the equation of the desired line be y=a+bx.\n",
    "\n",
    "To minimize: E=∑i(yi−a−bxi)2\n",
    "Differentiate E w.r.t a and b, set both of them to be equal to zero and solve for a and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is the p-value? How does it help in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => The p-value for each independent variable tests the null hypothesis that the variable has no correlation with the dependent variable. If there is no correlation, there is no association between the changes in the independent variable and the shifts in the dependent variable. In other words, there is insufficient evidence to conclude that there is effect at the population level.\n",
    "\n",
    "If the 95% confidence interval includes zero, the p-value for that coefficient will be greater than 0.05. If the 95% confidence interval does not include zero, the p-value will be less than 0.05.\n",
    "\n",
    "Thus, a p-value of less than 0.05 is a way to decide whether there is any relationship between the feature in consideration and the response or not. Using 0.05 as the cutoff is just a convention.\n",
    "for e.g In this case, the p-value for TV ads is way less than 0.05, and so we believe that there is a relationship between TV advertisements and Sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tHow to handle categorical values in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is absolutely critical that we make sure to encode categorical variables correctly, before we feed data into a machine learning algorithm.\n",
    "\n",
    "1) Mapping ordinal features : Unfortunately, there is no convenient function that can automatically derive the correct order of the labels of our size feature, so we have to define the mapping manually\n",
    "2) Encoding class labels : Many machine learning libraries require that class labels are encoded as integer values. To encode the class labels, we can use an approach similar to the mapping of ordinal features discussed previously. We need to remember that class labels are not ordinal, and it doesn’t matter which integer number we assign to a particular string label.\n",
    "3) one-hot encoding on nominal features : The idea behind this approach is to create a new dummy feature for each unique value in the nominal feature column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is regularization, and why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => When we use regression models to train some data, there is a good chance that the model will overfit the given training data set. Regularization helps sort this overfitting problem by restricting the degrees of freedom of a given equation.\n",
    "In a linear equation, we do not want huge weights/coefficients as a small change in weight can make a large difference for the dependent variable (Y). So, regularization constraints the weights of such features to avoid overfitting\n",
    "\n",
    "Regularization helps to reduce the variance of the model, without a substantial increase in the bias. If there is variance in the model that means that the model won’t fit well for dataset different that training data. The tuning parameter λ controls this bias and variance tradeoff. When the value of λ is increased up to a certain limit, it reduces the variance without losing any important properties in the data. But after a certain limit, the model will start losing some important properties which will increase the bias in the data. Thus, the selection of good value of λ is the key. The value of λ is selected using cross-validation methods. A set of λ is selected and cross-validation error is calculated for each value of λ and that value of λ is selected for which the cross-validation error is minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tExplain Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => Ridge regression penalizes the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by regularization= 𝜆∗∑|𝛽2𝑗|\n",
    "Ridge regression shrinks the coefficients for those predictors which contribute very less in the model but have huge weights, very close to zero. But it never makes them exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tExplain Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => LASSO regression penalizes the model based on the sum of magnitude of the coefficients. The regularization term is given by regularization= 𝜆∗∑|𝛽𝑗|\n",
    "\n",
    "This is where Lasso regression differs with Ridge regression. In Lasso, the L1 penalty does reduce some coefficients exactly to zero when we use a sufficiently large tuning parameter λ. So, in addition to regularizing, lasso also performs feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tExplain Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio α.\n",
    "\n",
    "where α is the mixing parameter between ridge (α = 0) and lasso (α = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhy do we do a train test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans => The data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model's prediction on this subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30.\tWhat is polynomial regression? When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans =>The goal of polynomial regression is to model a non-linear relationship between the independent and dependent variables\n",
    "So, Polynomial Regression can be defined as a mechanism to predict a dependent variable based on the polynomial relationship with the independent variable.\n",
    "Sometimes relationship between X and Y can't be described Linearly. Then comes the time to use the Polynomial Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.\tExplain the steps for GCP deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:-\n",
    "1) create account in GCP Then go to the console of your account, Go to IAM and admin(highlighted) and click manage resources\n",
    " Click CREATE PROJECT to create a new project for deployment, Once the project gets created, select App Engine and select Dashboard, download the google cloud SDK to your machine, Click Start Tutorial on the screen and select Python app and click start, Check whether the correct project name is displayed and then click next.\n",
    "\n",
    "3) Create a file ‘app.yaml’ and put ‘runtime: python37’ in that file. Create a ‘requirements.txt’ file by opening the command prompt/anaconda prompt, navigate to the project folder and enter the command ‘pip freeze > requirements.txt’. It is recommended to use separate environments for different projects. Your python application file should be called ‘main.py’. It is a GCP specific requirement. Open command prompt window, navigate to the project folder and enter the command gcloud init to initialise the gcloud context.It asks you to select from the list of available projects.\n",
    "\n",
    "4) Once the project name is selected, enter the command gcloud app deploy app.yaml --project After executing the above command, GCP will ask you to enter the region for your application. Choose the appropriate one.\n",
    "\n",
    "5) GCP will ask for the services to be deployed. Enter ‘y’ to deploy the services. And then it will give you the link for your app,and the deployed app looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. What difficulties did you face in cloud deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ‘requirements.txt’"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
